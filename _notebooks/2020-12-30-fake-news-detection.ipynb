{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#  Detect Fake News - Using a variety of scikit-learn classifiers\n",
    "> Given a news data set, distinquish real news from fake news. \n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jupyter, sklearn, tfidfvectorizer, passiveaggresiveclassifier, logisticregressionclassifier, supportvectorclassifier, naivebayesclassifier, decisiontreeclassifier, randomforestclassifier, mlpclassifier]\n",
    "- author: Venkataramani, Suja"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "\n",
    "Given a corpus of news documents each labelled as \"Real\" or \"Fake\", the task is to predict the correct label. We will run a variety of ML classifiers to predict the output and compare the accuracy of the classifiers. The input documents will be split into training and test set, transformed to tf-idf vector before feeding them to the classifiers. We will also attempt to understand different concepts and classifiers along the way."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method\n",
    "\n",
    "First, let's download [news data](https://drive.google.com/file/d/1er9NJTLUA3qnRuyhfzuN0XUsoIC4a-_q/view) set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "news_data = pd.read_csv(\".\\\\data\\\\news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6335, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Check te number of rows and columns in the dataset.\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8476</td>\n      <td>You Can Smell Hillary’s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10294</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3608</td>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>REAL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10142</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>875</td>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>REAL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Check the first 5 rows.\n",
    "news_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data by column to input and output.\n",
    "x = news_data['text']\n",
    "y = news_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the x and y into 80% train data and 20% test data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "source": [
    "### TF-IDF - What is the intuition behind it?\n",
    "The aim is to convert a document of words into a vector of numbers so it can be fed to ML algorithms. For every word in every document tf-idf is calculated to show the importance of the word in the corpus and the document.\n",
    "\n",
    "t = term, d = document, N = number of documents in the corpus\n",
    "\n",
    "- Term Frequency: Calculate the frequency of a word in a document.\n",
    "tf (t, d) = count of the word in the document / total number of words in the document\n",
    "\n",
    "- Document Frequency: Calculate the the number of documents in which the term occurs.\n",
    "df(t) = count of the documents in which term occurs.\n",
    "\n",
    "- Inverse Document Frequency: Measures the importance of the word in the corpus, relative weight of the term.\n",
    "idf(t) = N/ df(t) - When the corpus is large, the number can be large.\n",
    "idf(t) = log (N/df(t) + 1) - Taking a log dampens this large value, +1 in the denominator is to avoid division by 0.\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency:\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "\n",
    "[MonkeyLearn](https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20is%20a%20statistical,across%20a%20set%20of%20documents.)  \n",
    "[TowardsDataScience](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# stop_words = 'english': Removes all common uninformative words like and, the, etc.\n",
    "# max_df = 0.7 : Removes corpus-specific stop words when the document frequecy is above 0.7. \n",
    "tf_idf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n",
    "\n",
    "# fit_transform - Learns the terms and returns a document-term sparce matrix (n_samples, n_features)\n",
    "tf_idf_x_train = tf_idf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# transform - Returns a document-term sparce matrix (n_samples, n_features)\n",
    "tf_idf_x_test = tf_idf_vectorizer.transform(x_test)"
   ]
  },
  {
   "source": [
    "### 1. Passive Aggressive Classifier - What is it?\n",
    "\n",
    "PAC is a streaming algorithm for classifying massive amounts of data. Data is fed to the algorithm in sequestial order. For every example, if the prediction is correct, no change is made to the model (passive), but if the the prediction is incorrect, the model weight is changed to correct the model. The example is thrown away after processing.\n",
    "\n",
    "This works on the principle of Hinge loss, where if the prediction is correct or overly correct, there is no loss, but if the change is incorrect then there is a loss, bigger the difference, bigger the loss.\n",
    "\n",
    "The loss is added to the weight vector such that the prediction is just equal to 1.\n",
    "\n",
    "[GeeksForGeeks](https://www.geeksforgeeks.org/passive-aggressive-classifiers/)  \n",
    "[YouTube](https://www.youtube.com/watch?v=TJU8NfDdqNQ)   \n",
    "[MachineCurve](https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PAC Accuracy =  93.291\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit PAC.\n",
    "pac = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pac.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "# Predict on the test set.\n",
    "y_pred = pac.predict(tf_idf_x_test)\n",
    "\n",
    "# Calculate the accuracy.\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"PAC Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 2. Logistic Regression Classifier\n",
    "\n",
    "Logistic regression is like linear regression but with binomial (true/false) results. It predicts the probability of the class using a sigmoid function. \n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)   \n",
    "[YouTube](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR Accuracy =  90.845\n"
     ]
    }
   ],
   "source": [
    "# Random state - controls the random number generator, multiple calls to the funtion will reproduce same result.\n",
    "lr = LogisticRegression(random_state = 100)\n",
    "lr.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"LR Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 3. Support Vector Machine\n",
    "\n",
    "SVM is a supervised ML model which can be used for classification. It does this by finding the best line/plane of separation between the nearest data points from each class. If the classes are not linearly separable in the given dimensions, new dimension is calculated using a kernel function and then a hyperplane is calculated.\n",
    "\n",
    "[MonkeyLearn](https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/)    \n",
    "[YouTube](https://www.youtube.com/watch?v=efR1C6CvhmE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR Accuracy =  50.039\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"SVC Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 4. Naive Bayes Classifier\n",
    "\n",
    "NB is a probabilistic ML model based on Bayes theorem:\n",
    "\n",
    "P(y|X) =  (P(X|y)p(y))/(P(X))\n",
    "\n",
    "Where X = x1, x2...xn (n features), y = expected result.\n",
    "All the predictors are expected to be unrelated (naive assumption) and are considered equally important.\n",
    "\n",
    "[TowardsDataScience](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c)  \n",
    "[YouTube](https://www.youtube.com/watch?v=O2L2Uv9pdDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MNB Accuracy =  85.556\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = mnb.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"MNB Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 5. Decision Tree Classifier\n",
    "\n",
    "DT is a non-parametric (data does not have well defined distribution) supervised model. This model creates a tree with the given data. When new data is presented, it follows the decision tree to arrive at a class.\n",
    "\n",
    "[YouTube](https://www.youtube.com/watch?v=7VeUPuFGJHk)    \n",
    "[Scikit-Learn](https://scikit-learn.org/stable/modules/tree.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNB Accuracy =  80.9\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = dtc.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"DTC Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 6. Random Forest Classifier\n",
    "\n",
    "This is a ensemble () classifier where several sub-samples of the data (bootstrapping - random sampling of data with replacement) is used to build decision tree classifiers and the result is calculated based on the average of the results from the different trees.\n",
    "\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#)    \n",
    "[YouTube](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNB Accuracy =  85.162\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"RFC Accuracy = \", round((score * 100), 3))"
   ]
  },
  {
   "source": [
    "### 7. Multilayer Perceptron Classifier\n",
    "\n",
    "A neural network model based on perceptrons with hidden layers and back propogation. The hidden layers help learn complex patterns int he data and back propogation adjusts the weights of the classifier after every iteration to minimise loss.\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)    \n",
    "[TowardsDataScience](https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RFC Accuracy =  93.212\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=100, max_iter=100)\n",
    "mlp.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(tf_idf_x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"RFC Accuracy = \", round((score * 100), 3))   "
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "Both PassiveAggressiveClassifier and MLPClassifier resulted in **93%** accuracy while SVCClassifier produced just 50% accuracy. RandomForestClassifier had a better accuracy (85%) than decision trees(80%). MLP results can probably be improved with hyperparameter tuning. None of the classifiers were tuned for optimal performance and we have not measured the time it takes to train the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}