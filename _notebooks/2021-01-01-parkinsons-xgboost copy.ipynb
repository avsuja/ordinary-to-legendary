{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Detect Parkinsons - Using XGBoost Classifier\n",
    "> Using XGBoost to classify Parkinsons disease.\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jupyter, sklearn, xgboost]\n",
    "- author: Venkataramani, Suja"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "\n",
    "XGBoost is perfectly suited to large datasets with numerous features with a mixture of categorical and numerical features for non-deep learning problems. While our dataset is quite small, for the purposes of this example, we will use XGBoost. \n",
    "\n",
    "XGBoost does not need feature scaling/normalisation as xgboost is a ensemble of decision trees and distance between features are not used in the algorithm (unlike KNN, PCA).\n",
    "\n",
    "We begin by splitting the dependent(Y) and independent (X) variables. There are 23 independent variables and \"Status\" is the label, the dependent variable. Then we split the dataset into training and test set. We fit XGBoost model with the training set and test with the test set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method\n",
    "\n",
    "First, let's download the Parkingsons data set from [UCI Machine Learning](https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/).  \n",
    "\n",
    "Install xgboost for Python with \"pip install xgboost\" at command prompt. Test with python \"import xgboost\" to make sure all is well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "pk_data = pd.read_csv(\".\\\\data\\\\parkinsons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(195, 24)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Check te number of rows and columns in the dataset.\n",
    "pk_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer    ...     \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374    ...      \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134    ...      \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233    ...      \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492    ...      \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425    ...      \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>MDVP:Fo(Hz)</th>\n      <th>MDVP:Fhi(Hz)</th>\n      <th>MDVP:Flo(Hz)</th>\n      <th>MDVP:Jitter(%)</th>\n      <th>MDVP:Jitter(Abs)</th>\n      <th>MDVP:RAP</th>\n      <th>MDVP:PPQ</th>\n      <th>Jitter:DDP</th>\n      <th>MDVP:Shimmer</th>\n      <th>...</th>\n      <th>Shimmer:DDA</th>\n      <th>NHR</th>\n      <th>HNR</th>\n      <th>status</th>\n      <th>RPDE</th>\n      <th>DFA</th>\n      <th>spread1</th>\n      <th>spread2</th>\n      <th>D2</th>\n      <th>PPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>phon_R01_S01_1</td>\n      <td>119.992</td>\n      <td>157.302</td>\n      <td>74.997</td>\n      <td>0.00784</td>\n      <td>0.00007</td>\n      <td>0.00370</td>\n      <td>0.00554</td>\n      <td>0.01109</td>\n      <td>0.04374</td>\n      <td>...</td>\n      <td>0.06545</td>\n      <td>0.02211</td>\n      <td>21.033</td>\n      <td>1</td>\n      <td>0.414783</td>\n      <td>0.815285</td>\n      <td>-4.813031</td>\n      <td>0.266482</td>\n      <td>2.301442</td>\n      <td>0.284654</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>phon_R01_S01_2</td>\n      <td>122.400</td>\n      <td>148.650</td>\n      <td>113.819</td>\n      <td>0.00968</td>\n      <td>0.00008</td>\n      <td>0.00465</td>\n      <td>0.00696</td>\n      <td>0.01394</td>\n      <td>0.06134</td>\n      <td>...</td>\n      <td>0.09403</td>\n      <td>0.01929</td>\n      <td>19.085</td>\n      <td>1</td>\n      <td>0.458359</td>\n      <td>0.819521</td>\n      <td>-4.075192</td>\n      <td>0.335590</td>\n      <td>2.486855</td>\n      <td>0.368674</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>phon_R01_S01_3</td>\n      <td>116.682</td>\n      <td>131.111</td>\n      <td>111.555</td>\n      <td>0.01050</td>\n      <td>0.00009</td>\n      <td>0.00544</td>\n      <td>0.00781</td>\n      <td>0.01633</td>\n      <td>0.05233</td>\n      <td>...</td>\n      <td>0.08270</td>\n      <td>0.01309</td>\n      <td>20.651</td>\n      <td>1</td>\n      <td>0.429895</td>\n      <td>0.825288</td>\n      <td>-4.443179</td>\n      <td>0.311173</td>\n      <td>2.342259</td>\n      <td>0.332634</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>phon_R01_S01_4</td>\n      <td>116.676</td>\n      <td>137.871</td>\n      <td>111.366</td>\n      <td>0.00997</td>\n      <td>0.00009</td>\n      <td>0.00502</td>\n      <td>0.00698</td>\n      <td>0.01505</td>\n      <td>0.05492</td>\n      <td>...</td>\n      <td>0.08771</td>\n      <td>0.01353</td>\n      <td>20.644</td>\n      <td>1</td>\n      <td>0.434969</td>\n      <td>0.819235</td>\n      <td>-4.117501</td>\n      <td>0.334147</td>\n      <td>2.405554</td>\n      <td>0.368975</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>phon_R01_S01_5</td>\n      <td>116.014</td>\n      <td>141.781</td>\n      <td>110.655</td>\n      <td>0.01284</td>\n      <td>0.00011</td>\n      <td>0.00655</td>\n      <td>0.00908</td>\n      <td>0.01966</td>\n      <td>0.06425</td>\n      <td>...</td>\n      <td>0.10470</td>\n      <td>0.01767</td>\n      <td>19.649</td>\n      <td>1</td>\n      <td>0.417356</td>\n      <td>0.823484</td>\n      <td>-3.747787</td>\n      <td>0.234513</td>\n      <td>2.332180</td>\n      <td>0.410335</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Check the first 5 rows.\n",
    "pk_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc[row_from: row_to, boolean list of columns to keep/leave]\n",
    "# pk_data.columns != 'status' -> Gives an array of bool values, true for all but status. \n",
    "# .values - converts df into array.\n",
    "x = pk_data.loc[:, pk_data.columns != 'status'].values[:, 1:]\n",
    "# loc[row_from: row_to, column_from: column_to]\n",
    "y = pk_data.loc[:,'status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the x and y into 80% train data and 20% test data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "source": [
    "### XGBoost \n",
    "\n",
    "XGB is a progression of Decision Trees. When bagging (taking random samples with replacement from the data) is applied to decision trees it results in Random Forest which is a ensemble of decision trees which results in better accuracy. When boosting (when every tree built aims to correct the errors in the previous tree, additive trees) is added to Random Forest, it result in Boosted Random Tree. When the errors are minimised using Gradient Descent, it results in Gradient Boosting. \n",
    "\n",
    "XGB added further optimisations to this Gradient Boosted Trees such as parallel processing, tree pruning (to avoid being penalised by regularisation term) along with an efficient missing value imputation and cross-validation. Along with the algorithmic advances the hardware is optimised resulting in significant performance improvements. \n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)    \n",
    "[TowardsDataScience](https://towardsdatascience.com/)  \n",
    "[Medium](https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)  \n",
    "[MachineLearningMastery](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)  \n",
    "[YouTube-GradientBoost](https://www.youtube.com/watch?v=3CC4N4z3GJc)  \n",
    "[YouTube-XGBoost](https://www.youtube.com/watch?v=OtD8wVaFm6E)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[21:21:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGB Accuracy:  94.872\n"
     ]
    }
   ],
   "source": [
    "xgbc = xgb.XGBClassifier()\n",
    "xgbc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = xgbc.predict(x_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('XGB Accuracy: ', round(score * 100, 3))"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "Even though the dataset is not massive, XBBoost performed well with **94%** accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}