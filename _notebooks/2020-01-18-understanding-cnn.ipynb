{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Improving CNN (Pytorch) \n",
    "> Classify digit images using Convolutional Neural Network. \n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jupyter, pytorch, neuralnetwork, convolutionalneuralnetwork]\n",
    "- author: Venkataramani, Suja"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.nn import Linear, ReLU, Sequential, Conv2d, MaxPool2d, Dropout, BatchNorm2d\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tranforms\" method aids image tranformations. A set of tranformations can be chained together using Compose.\n",
    "\n",
    "# ToTensor converts a numpy image array of (H, W, C) in the range (0, 255) in to a tensor of (C, H, W) in the range (0, 1)\n",
    "\n",
    "# Normalize method accepts mean and std deviation as input. For every channel performs (image - mean)/std. this arranges all the numbers of the channel within the same range and reduces the skews in input data dute to different ranges of numbers.\n",
    "transform_step = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# TorchVision.datasets has the most commonly used deep learning datasets available for easy download. All datasets have common interface of tranform. train=True gets the training data (60,000 samples), train=False gets test data (10,00 samples).\n",
    "train_data = datasets.MNIST(root='./data/mmist_train', download=True, train=True, transform=transform_step)\n",
    "test_data = datasets.MNIST(root='./data/minst_test', download=True, train=False, transform=transform_step)\n",
    "\n",
    "# DataLoader creates a iterable batches of data in order to aid with training a nn model. Setting shuffle to True results in a random suffled batch of images.\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "source": [
    "### What is Regularization?\n",
    "\n",
    "When a model learns the training data so well that it has not learnt the pattern but instead the data itself, the model tends to not generalize well, i.e. it does not predict the test data well. Regularization is a penalty term added to the error term such that the co-efficients learnt by this model does not fluctuate wildly. \n",
    "\n",
    "In Neural Networks there are several techniques for applying  regularization:\n",
    "1. Dropout: This is implemented as a layer where the parameters are dropped randomly with a probablity of retaining the values. This has the effect forcing the model to learn from sparse neurons. Probability of 1 means no dropouts, 0.2 means 0.2 of the the neurons will be dropped.  \n",
    "2. L1 Regularization (Lasso Regression): This regression is applied such that the the weights are sparse and therefore the model has fewer parameters. Only the important parameters are retained and the co-efficient of less important parameters are reduced to zero, thus learning the general trend of the model. (nn.L1Loss)  \n",
    "3. L2 Regularization (Ridge Regression): In this the squared magnitude of the cofficient is added to the penalty term, leaving us with a set of simple features that explain the model most, the less important ones have lesser weights.  \n",
    "\n",
    "[TowardsDataScience](https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea#:~:text=Regularization%20is%20a%20technique%20used,don't%20take%20extreme%20values.)  \n",
    "[EddenGerber] (https://medium.com/@edden.gerber/thanks-for-the-article-1003ad7478b2)  \n",
    "[MachineLearningMastery](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New class extending the nn.Module - the base class of all neural networks.\n",
    "class Net_d(nn.Module):\n",
    "    # Constructor which first calls the base class constructor.\n",
    "    def __init__(self):\n",
    "        super(Net_d, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=4, kernel_size=5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=2),\n",
    "            Dropout(p=0.5),            \n",
    "            Conv2d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            ReLU(),            \n",
    "            MaxPool2d(kernel_size=2),\n",
    "            Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "        # Final fully connected layer.\n",
    "        self.linear_layers = Sequential(\n",
    "            # Input and output.\n",
    "            Linear(4 * 4 * 4, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(-1, 4 * 4 * 4)\n",
    "        x = self.linear_layers(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Instantiate dropout CNN.\n",
    "cnn_model = Net_d()\n",
    "\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "cnn_criterion = nn.NLLLoss()\n",
    "cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs = 5\n",
    "\n",
    "for e in range (epochs):\n",
    "    for images, labels in train_data_loader:\n",
    "        \n",
    "        cnn_optimizer.zero_grad()\n",
    "        # Forward.\n",
    "        log_prob = cnn_model(images)\n",
    "        loss = cnn_criterion(log_prob, labels)\n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        # Optimize.\n",
    "        cnn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the test set.\n",
    "correct_count = 0\n",
    "count = 0\n",
    "\n",
    "for data in test_data_loader:\n",
    "    # 32 images and labels.\n",
    "    images, labels = data\n",
    "\n",
    "    # Get the predictions.\n",
    "    # 32 outputs with log probabilities of 10 each for each of the 10 digits.\n",
    "    with torch.no_grad():\n",
    "        outputs = cnn_model(images)\n",
    "\n",
    "    # torch.max - with dim=1 (column) results in max of the probablilities for each of 32 images. It returns 2 values - max probabliltiy and            max index, we are interested in the max index. _, is used to ignore the first set of output.\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Get the number of images - 32 in each batch except for the last batch.\n",
    "    count += labels.size(0)\n",
    "\n",
    "    # Get the number of correct guesses in this batch.\n",
    "    correct_count += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97.92999999999999\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_count/count) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "### Dropout did not work well for our CNN. Why?\n",
    "Using this CNN class, we got an accuracy of 81.69. Dropouts are not always effective, they are more effective when the training data - some of the reasons are using dropouts as the last step in NN gives the model no means to correct itself, when the network is small compared to the training data size, when there are not enough epochs to give reach convergence are all cited as reasons for this. \n",
    "\n",
    "Other reasons CNN particularly does not take to droput has been observed as high level of correlation between activations and use of max pooling to reduce the number of parameters. \n",
    "\n",
    "[StackExchange](https://stats.stackexchange.com/questions/299292/dropout-makes-performance-worse)  \n",
    "[KDNuggets](https://www.kdnuggets.com/2018/09/dropou9t-convolutional-networks.html)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Batch Normalization\n",
    "\n",
    "When a model is made of tens of layers, and data is fed in batches, the weight inputs a a layer get adjusted after every batch, which causes the model constantly readjust its weights and take longer to converge. This is called internal covariate shift. To counteract this, the learning rates need to be reduced or the initial parameters must be selected with care. This also means higher epochs to achieve a good model convergence.\n",
    "\n",
    "Batch normalization is technique where the input to a layer is normalised such than mean is 0 and standard deviation of 1, which calms the constant shift in weights and helps achieve convergence faster. This also allows us to get rid of dropout layers.  \n",
    "\n",
    "[MachineCurve]https://www.machinecurve.com(/index.php/2020/01/14/what-is-batch-normalization-for-training-neural-networks/)\n",
    "[MachineLearningMastery](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)  \n",
    "[AIWorkBox](https://www.aiworkbox.com/lessons/batchnorm2d-how-to-use-the-batchnorm2d-module-in-pytorch)  \n",
    "[OrignalPaper](https://arxiv.org/abs/1502.03167)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class with Batch normalisation.\n",
    "class Net_b(nn.Module):\n",
    "    # Constructor which first calls the base class constructor.\n",
    "    def __init__(self):\n",
    "        super(Net_b, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=4, kernel_size=5),\n",
    "            BatchNorm2d(num_features=4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=2),\n",
    "            Conv2d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            BatchNorm2d(num_features=4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            ReLU(),            \n",
    "            MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # Final fully connected layer.\n",
    "        self.linear_layers = Sequential(\n",
    "            # Input and output.\n",
    "            Linear(4 * 4 * 4, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(-1, 4 * 4 * 4)\n",
    "        x = self.linear_layers(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net_b(\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Instantiate the batch norm CNN.\n",
    "cnn_model = Net_b()\n",
    "\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97.63\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_count/count) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "### Conclusion\n",
    "\n",
    "We learnt important concepts such a dropout and batch normalization and applied it to our CNN. Neither actually performed better than the original CNN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}