{
  
    
        "post0": {
            "title": "Detect Parkinsons - Using XGBoost Classifier",
            "content": "Overview . XGBoost is perfectly suited to large datasets with numerous features with a mixture of categorical and numerical features for non-deep learning problems. While our dataset is quite small, for the purposes of this example, we will use XGBoost. . XGBoost does not need feature scaling/normalisation as xgboost is a ensemble of decision trees and distance between features are not used in the algorithm (unlike KNN, PCA). . We begin by splitting the dependent(Y) and independent (X) variables. There are 23 independent variables and &quot;Status&quot; is the label, the dependent variable. Then we split the dataset into training and test set. We fit XGBoost model with the training set and test with the test set. . Method . First, let&#39;s download the Parkingsons data set from UCI Machine Learning. . Install xgboost for Python with &quot;pip install xgboost&quot; at command prompt. Test with python &quot;import xgboost&quot; to make sure all is well. . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix import xgboost as xgb . pk_data = pd.read_csv(&quot;. data parkinsons.csv&quot;) . pk_data.shape . (195, 24) . pk_data.head(5) . name MDVP:Fo(Hz) MDVP:Fhi(Hz) MDVP:Flo(Hz) MDVP:Jitter(%) MDVP:Jitter(Abs) MDVP:RAP MDVP:PPQ Jitter:DDP MDVP:Shimmer ... Shimmer:DDA NHR HNR status RPDE DFA spread1 spread2 D2 PPE . 0 phon_R01_S01_1 | 119.992 | 157.302 | 74.997 | 0.00784 | 0.00007 | 0.00370 | 0.00554 | 0.01109 | 0.04374 | ... | 0.06545 | 0.02211 | 21.033 | 1 | 0.414783 | 0.815285 | -4.813031 | 0.266482 | 2.301442 | 0.284654 | . 1 phon_R01_S01_2 | 122.400 | 148.650 | 113.819 | 0.00968 | 0.00008 | 0.00465 | 0.00696 | 0.01394 | 0.06134 | ... | 0.09403 | 0.01929 | 19.085 | 1 | 0.458359 | 0.819521 | -4.075192 | 0.335590 | 2.486855 | 0.368674 | . 2 phon_R01_S01_3 | 116.682 | 131.111 | 111.555 | 0.01050 | 0.00009 | 0.00544 | 0.00781 | 0.01633 | 0.05233 | ... | 0.08270 | 0.01309 | 20.651 | 1 | 0.429895 | 0.825288 | -4.443179 | 0.311173 | 2.342259 | 0.332634 | . 3 phon_R01_S01_4 | 116.676 | 137.871 | 111.366 | 0.00997 | 0.00009 | 0.00502 | 0.00698 | 0.01505 | 0.05492 | ... | 0.08771 | 0.01353 | 20.644 | 1 | 0.434969 | 0.819235 | -4.117501 | 0.334147 | 2.405554 | 0.368975 | . 4 phon_R01_S01_5 | 116.014 | 141.781 | 110.655 | 0.01284 | 0.00011 | 0.00655 | 0.00908 | 0.01966 | 0.06425 | ... | 0.10470 | 0.01767 | 19.649 | 1 | 0.417356 | 0.823484 | -3.747787 | 0.234513 | 2.332180 | 0.410335 | . 5 rows × 24 columns . # pk_data.columns != &#39;status&#39; -&gt; Gives an array of bool values, true for all but status. # .values - converts df into array. x = pk_data.loc[:, pk_data.columns != &#39;status&#39;].values[:, 1:] # loc[row_from: row_to, column_from: column_to] y = pk_data.loc[:,&#39;status&#39;].values . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100) . XGBoost . XGB is a progression of Decision Trees. When bagging (taking random samples with replacement from the data) is applied to decision trees it results in Random Forest which is a ensemble of decision trees which results in better accuracy. When boosting (when every tree built aims to correct the errors in the previous tree, additive trees) is added to Random Forest, it result in Boosted Random Tree. When the errors are minimised using Gradient Descent, it results in Gradient Boosting. . XGB added further optimisations to this Gradient Boosted Trees such as parallel processing, tree pruning (to avoid being penalised by regularisation term) along with an efficient missing value imputation and cross-validation. Along with the algorithmic advances the hardware is optimised resulting in significant performance improvements. . XGBoost TowardsDataScience Medium MachineLearningMastery YouTube-GradientBoost YouTube-XGBoost . xgbc = xgb.XGBClassifier() xgbc.fit(x_train, y_train) y_pred = xgbc.predict(x_test) score = accuracy_score(y_test, y_pred) print(&#39;XGB Accuracy: &#39;, round(score * 100, 3)) . [21:21:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. XGB Accuracy: 94.872 . Conclusion . Even though the dataset is not massive, XBBoost performed well with 94% accuracy. .",
            "url": "https://avsuja.github.io/ordinary-to-legendary/jupyter/sklearn/xgboost/2021/01/01/parkinsons-xgboost.html",
            "relUrl": "/jupyter/sklearn/xgboost/2021/01/01/parkinsons-xgboost.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Detect Fake News - Using a variety of scikit-learn classifiers",
            "content": "Overview . Given a corpus of news documents each labelled as &quot;Real&quot; or &quot;Fake&quot;, the task is to predict the correct label. We will run a variety of ML classifiers to predict the output and compare the accuracy of the classifiers. The input documents will be split into training and test set, transformed to tf-idf vector before feeding them to the classifiers. We will also attempt to understand different concepts and classifiers along the way. . Method . First, let&#39;s download news data set. . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import PassiveAggressiveClassifier from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.naive_bayes import MultinomialNB from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.neural_network import MLPClassifier . news_data = pd.read_csv(&quot;. data news.csv&quot;) . news_data.shape . (6335, 4) . news_data.head(5) . Unnamed: 0 title text label . 0 8476 | You Can Smell Hillary’s Fear | Daniel Greenfield, a Shillman Journalism Fello... | FAKE | . 1 10294 | Watch The Exact Moment Paul Ryan Committed Pol... | Google Pinterest Digg Linkedin Reddit Stumbleu... | FAKE | . 2 3608 | Kerry to go to Paris in gesture of sympathy | U.S. Secretary of State John F. Kerry said Mon... | REAL | . 3 10142 | Bernie supporters on Twitter erupt in anger ag... | — Kaydee King (@KaydeeKing) November 9, 2016 T... | FAKE | . 4 875 | The Battle of New York: Why This Primary Matters | It&#39;s primary day in New York and front-runners... | REAL | . x = news_data[&#39;text&#39;] y = news_data[&#39;label&#39;] . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100) . TF-IDF - What is the intuition behind it? . The aim is to convert a document of words into a vector of numbers so it can be fed to ML algorithms. For every word in every document tf-idf is calculated to show the importance of the word in the corpus and the document. . t = term, d = document, N = number of documents in the corpus . Term Frequency: Calculate the frequency of a word in a document. tf (t, d) = count of the word in the document / total number of words in the document . | Document Frequency: Calculate the the number of documents in which the term occurs. df(t) = count of the documents in which term occurs. . | Inverse Document Frequency: Measures the importance of the word in the corpus, relative weight of the term. idf(t) = N/ df(t) - When the corpus is large, the number can be large. idf(t) = log (N/df(t) + 1) - Taking a log dampens this large value, +1 in the denominator is to avoid division by 0. . | Term Frequency-Inverse Document Frequency: tf-idf(t, d) = tf(t, d) * idf(t) . | . MonkeyLearn TowardsDataScience . # stop_words = &#39;english&#39;: Removes all common uninformative words like and, the, etc. # max_df = 0.7 : Removes corpus-specific stop words when the document frequecy is above 0.7. tf_idf_vectorizer = TfidfVectorizer(stop_words = &#39;english&#39;, max_df = 0.7) # fit_transform - Learns the terms and returns a document-term sparce matrix (n_samples, n_features) tf_idf_x_train = tf_idf_vectorizer.fit_transform(x_train) # transform - Returns a document-term sparce matrix (n_samples, n_features) tf_idf_x_test = tf_idf_vectorizer.transform(x_test) . 1. Passive Aggressive Classifier - What is it? . PAC is a streaming algorithm for classifying massive amounts of data. Data is fed to the algorithm in sequestial order. For every example, if the prediction is correct, no change is made to the model (passive), but if the the prediction is incorrect, the model weight is changed to correct the model. The example is thrown away after processing. . This works on the principle of Hinge loss, where if the prediction is correct or overly correct, there is no loss, but if the change is incorrect then there is a loss, bigger the difference, bigger the loss. . The loss is added to the weight vector such that the prediction is just equal to 1. . GeeksForGeeks YouTube MachineCurve . pac = PassiveAggressiveClassifier(max_iter = 50) pac.fit(tf_idf_x_train, y_train) # Predict on the test set. y_pred = pac.predict(tf_idf_x_test) # Calculate the accuracy. score = accuracy_score(y_test, y_pred) print(&quot;PAC Accuracy = &quot;, round((score * 100), 3)) . PAC Accuracy = 93.291 . 2. Logistic Regression Classifier . Logistic regression is like linear regression but with binomial (true/false) results. It predicts the probability of the class using a sigmoid function. . Scikit-Learn YouTube . lr = LogisticRegression(random_state = 100) lr.fit(tf_idf_x_train, y_train) y_pred = lr.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;LR Accuracy = &quot;, round((score * 100), 3)) . LR Accuracy = 90.845 . 3. Support Vector Machine . SVM is a supervised ML model which can be used for classification. It does this by finding the best line/plane of separation between the nearest data points from each class. If the classes are not linearly separable in the given dimensions, new dimension is calculated using a kernel function and then a hyperplane is calculated. . MonkeyLearn YouTube . svc = SVC() svc.fit(tf_idf_x_train, y_train) y_pred = svc.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;SVC Accuracy = &quot;, round((score * 100), 3)) . LR Accuracy = 50.039 . 4. Naive Bayes Classifier . NB is a probabilistic ML model based on Bayes theorem: . P(y|X) = (P(X|y)p(y))/(P(X)) . Where X = x1, x2...xn (n features), y = expected result. All the predictors are expected to be unrelated (naive assumption) and are considered equally important. . TowardsDataScience YouTube . mnb = MultinomialNB() mnb.fit(tf_idf_x_train, y_train) y_pred = mnb.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;MNB Accuracy = &quot;, round((score * 100), 3)) . MNB Accuracy = 85.556 . 5. Decision Tree Classifier . DT is a non-parametric (data does not have well defined distribution) supervised model. This model creates a tree with the given data. When new data is presented, it follows the decision tree to arrive at a class. . YouTube Scikit-Learn . dtc = DecisionTreeClassifier() dtc.fit(tf_idf_x_train, y_train) y_pred = dtc.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;DTC Accuracy = &quot;, round((score * 100), 3)) . GNB Accuracy = 80.9 . 6. Random Forest Classifier . This is a ensemble () classifier where several sub-samples of the data (bootstrapping - random sampling of data with replacement) is used to build decision tree classifiers and the result is calculated based on the average of the results from the different trees. . Scikit-Learn YouTube . rfc = RandomForestClassifier() rfc.fit(tf_idf_x_train, y_train) y_pred = rfc.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;RFC Accuracy = &quot;, round((score * 100), 3)) . GNB Accuracy = 85.162 . 7. Multilayer Perceptron Classifier . A neural network model based on perceptrons with hidden layers and back propogation. The hidden layers help learn complex patterns int he data and back propogation adjusts the weights of the classifier after every iteration to minimise loss. . Scikit-Learn TowardsDataScience . mlp = MLPClassifier(random_state=100, max_iter=100) mlp.fit(tf_idf_x_train, y_train) y_pred = mlp.predict(tf_idf_x_test) score = accuracy_score(y_test, y_pred) print(&quot;RFC Accuracy = &quot;, round((score * 100), 3)) . RFC Accuracy = 93.212 . Conclusion . Both PassiveAggressiveClassifier and MLPClassifier resulted in 93% accuracy while SVCClassifier produced just 50% accuracy. RandomForestClassifier had a better accuracy (85%) than decision trees(80%). MLP results can probably be improved with hyperparameter tuning. None of the classifiers were tuned for optimal performance and we have not measured the time it takes to train the model. .",
            "url": "https://avsuja.github.io/ordinary-to-legendary/jupyter/sklearn/tfidfvectorizer/passiveaggresiveclassifier/logisticregressionclassifier/supportvectorclassifier/naivebayesclassifier/decisiontreeclassifier/randomforestclassifier/mlpclassifier/2020/12/30/fake-news-detection.html",
            "relUrl": "/jupyter/sklearn/tfidfvectorizer/passiveaggresiveclassifier/logisticregressionclassifier/supportvectorclassifier/naivebayesclassifier/decisiontreeclassifier/randomforestclassifier/mlpclassifier/2020/12/30/fake-news-detection.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "But first...",
            "content": "Aim: To gain confidence in Data Science related topics. . Method: . 1. To build Data Science projects in various topics. 2. To blog about the related topics and the project findings. .",
            "url": "https://avsuja.github.io/ordinary-to-legendary/jupyter/2020/12/30/but-first.html",
            "relUrl": "/jupyter/2020/12/30/but-first.html",
            "date": " • Dec 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Data Scientist at Pearson. Check out my LinkedIn profile. .",
          "url": "https://avsuja.github.io/ordinary-to-legendary/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://avsuja.github.io/ordinary-to-legendary/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}